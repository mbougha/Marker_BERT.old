{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Marked_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a01d7d4297964b4d9e3cc364e7e7c51a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_caa814dca3604a3297ef210de11c61c0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1c8fc63ced814ba5ad1226d2f2bae45e",
              "IPY_MODEL_c3164d6976034372a7a1f4f32e78e891"
            ]
          }
        },
        "caa814dca3604a3297ef210de11c61c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c8fc63ced814ba5ad1226d2f2bae45e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d07c37e86ba74be3b9be5f63cdbfb3be",
            "_dom_classes": [],
            "description": "Iteration:   0%",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 1302,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7e96b7243b647318c1fb7f2e0bc83e3"
          }
        },
        "c3164d6976034372a7a1f4f32e78e891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_73082573ee12488e90947feba7ef694f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1302 [00:04&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3be2f77456b43f183f8535f9fb2ec09"
          }
        },
        "d07c37e86ba74be3b9be5f63cdbfb3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e7e96b7243b647318c1fb7f2e0bc83e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "73082573ee12488e90947feba7ef694f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e3be2f77456b43f183f8535f9fb2ec09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbougha/MarkerBERT/blob/master/Marked_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hFm6ylUn8oSu",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpbpoe4yygpU",
        "colab_type": "text"
      },
      "source": [
        "**Firstly**, we need to set up Colab TPU running environment, verify a TPU device is succesfully connected and upload credentials to TPU for GCS bucket usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRx7coBby4Il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "username='mbougha'#@param {type:\"string\"}\n",
        "password='samy%232000'#@param {type:\"string\"}\n",
        "!test -d MarkerBERT || git clone https://{username}:{password}@github.com/{username}/MarkerBERT.git\n",
        "if not 'MarkerBERT' in sys.path:\n",
        "  sys.path += ['MarkerBERT']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C0eAqa5g13Y6",
        "colab": {}
      },
      "source": [
        "!pip install -r MarkerBERT/requirements_colab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e0b9571b-94e5-4728-cb0b-b3ade121dee5",
        "id": "RCbDvDYx13ZB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0-dev20200311'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdUFaYR4wwZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pt-66aNExJMK"
      },
      "source": [
        "# DATA FILES NEED TO BE IN GCS BUCKET SO THAT THE TPU CAN ACCESS IT \n",
        "THE FIRST RUN: THE TPU HAS NO ACCESS PERMISSIONS TO THE BUCKET SO U NEED TO CHANGE THE PERMISSIONS OF YOUR BUCKET MANUALLY TO ADD THE TPU ( THE NAME WILL BE IN THE EXCEPTION TEXT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJvKZN02w2j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['COLAB_SKIP_TPU_AUTH'] = '1'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FaSBpUMw6dZ",
        "colab_type": "code",
        "outputId": "12e57895-13d0-4918-b4b1-f622b0632a38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "project_id=\"windy-impact-273008\"#@param {type:\"string\"}\n",
        "bucket_name=\"mbougha2\"#@param {type:\"string\"}\n",
        "!gcloud config set project {project_id}\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJEo5ZakzPNo",
        "colab_type": "text"
      },
      "source": [
        "**Thirdly**, prepare for training:\n",
        "\n",
        "* Specify training data.\n",
        "* Specify BERT pretrained model\n",
        "* Specify GS bucket, create output directory for model checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NsGKuHdzWzZ",
        "colab_type": "code",
        "outputId": "4b053a66-4375-4f93-e43c-b00a02d2c89d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "MODEL_TYPE = \"bert\" #@param {type:\"string\"}\n",
        "MODEL_NAME = \"bert-base-uncased\" #@param {type:\"string\"}\n",
        "\n",
        "OUTPUT_DIR = \"gs://mbougha2/output_dir/\" #@param {type:\"string\"}\n",
        "assert OUTPUT_DIR, 'Must specify an existing GCS bucket name'\n",
        "tf.io.gfile.makedirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Now we need to specify the input data dir. Should contain the .tfrecord files \n",
        "# and the supporting query-docids mapping files.\n",
        "DATA_DIR = \"gs://mbougha2/trec_test_set\" #@param {type:\"string\"}\n",
        "print('***** Data directory: {} *****'.format(DATA_DIR))\n",
        "\n",
        "FILE_NAME= \"dataset_cord19.tf\" #@param {type:\"string\"}\n",
        "#FILE_NAME=\"dataset_trec_base.tf\"\n",
        "\n",
        "\n",
        "SET_NAME= \"trec\" #@param {type:\"string\"}\n",
        "\n",
        "# need to mount your drive and put the path to the directory containing the .h5 file\n",
        "CHKPT_PATH=\"/content/drive/My Drive/MyColabNotebooks/MarkedBERT/checkpoint\" #@param {type:\"string\"}\n",
        "\n",
        "STRATEGY= \"mu_mark_pass\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://mbougha2/output_dir/ *****\n",
            "***** Data directory: gs://mbougha2/trec_test_set *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHntuq7okBGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e25f9265-cb08-4aaf-88b0-0f0445b723d7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI_d4sfn0g-R",
        "colab_type": "text"
      },
      "source": [
        "**Now, we can start training/evaluating**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jBv9QA3X13Zj",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "import collections\n",
        "import datetime\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from absl import app, flags, logging\n",
        "from tqdm import  trange\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import (\n",
        "    TF2_WEIGHTS_NAME,\n",
        "    BertConfig,\n",
        "    BertTokenizer,\n",
        "    DistilBertConfig,\n",
        "    DistilBertTokenizer,\n",
        "    RobertaConfig,\n",
        "    RobertaTokenizer,\n",
        "    TFBertForSequenceClassification,\n",
        "    TFDistilBertForSequenceClassification,\n",
        "    TFRobertaForSequenceClassification,\n",
        ")\n",
        "#local modules\n",
        "from Processors import Robust04Processor, MsMarcoDocumentProcessor, MsMarcoPassageProcessor, DocumentHandle, DocumentSplitterHandle, PassageHandle, get_marker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "37v3LH-I13Zn",
        "colab": {}
      },
      "source": [
        "ALL_MODELS = sum(\n",
        "    (tuple(conf.pretrained_config_archive_map.keys()) for conf in (BertConfig, RobertaConfig, DistilBertConfig)), ()\n",
        ")\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    \"bert\": (BertConfig, TFBertForSequenceClassification, BertTokenizer),\n",
        "    \"roberta\": (RobertaConfig, TFRobertaForSequenceClassification, RobertaTokenizer),\n",
        "    \"distilbert\": (DistilBertConfig, TFDistilBertForSequenceClassification, DistilBertTokenizer),\n",
        "}\n",
        "\n",
        "#Arguments \n",
        "args=dict()\n",
        "\n",
        "args[\"data_dir\"]= DATA_DIR\n",
        "args[\"model_type\"]=MODEL_TYPE\n",
        "args[\"model_name_or_path\"]=MODEL_NAME\n",
        "args[\"output_dir\"]=OUTPUT_DIR\n",
        "args[\"transformer_checkpoints\"]= CHKPT_PATH\n",
        "args[\"max_seq_length\"]=512\n",
        "args[\"tpu\"]= f\"grpc://{os.environ['COLAB_TPU_ADDR']}\"\n",
        "args[\"do_train\"]= False\n",
        "args['do_eval']= True\n",
        "args[\"per_device_train_batch_size\"]= 16\n",
        "args['per_device_eval_batch_size']= 4\n",
        "args[\"max_steps\"]=100000\n",
        "args[\"warmup_steps\"]=10000\n",
        "args[\"learning_rate\"]=3e-6\n",
        "args[\"adam_epsilon\"]=1e-8\n",
        "args[\"logging_steps\"]=100\n",
        "args[\"seed\"]=42\n",
        "args[\"max_grad_norm\"]=1.0\n",
        "args[\"save_steps\"]=5000\n",
        "args[\"overwrite_output_dir\"]=False\n",
        "args[\"fp16\"]=False\n",
        "args[\"no_cuda\"]=False\n",
        "args['gpus']=None\n",
        "args['config_name']=None\n",
        "args['cache_dir']=None\n",
        "args['tokenizer_name']=None\n",
        "args['num_tpu_cores']=8\n",
        "args['do_predict']=False\n",
        "args['evaluate_during_training']=False\n",
        "args['do_lower_case']=False\n",
        "args['num_train_epochs']=1\n",
        "args['overwrite_cache']=False\n",
        "args['eval_all_checkpoints']=False\n",
        "args[\"msmarco_output\"]= True\n",
        "args[\"num_eval_docs\"] =1000\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h1BVqcDD13Zz",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(\n",
        "    args, strategy,  train_dataset, model, num_train_examples, train_batch_size\n",
        "):\n",
        "    if args[\"max_steps\"] > 0:\n",
        "        num_train_steps = args[\"max_steps\"] \n",
        "        args[\"num_train_epochs\"] = 1 # only consider the case where max_steps < one_epoch_steps\n",
        "    else:\n",
        "        num_train_steps = (\n",
        "            math.ceil(num_train_examples / train_batch_size)\n",
        "            * args[\"num_train_epochs\"]\n",
        "        )\n",
        "\n",
        "    with strategy.scope():\n",
        "        loss_fct = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
        "        optimizer = create_optimizer(args[\"learning_rate\"], num_train_steps, args[\"warmup_steps\"])\n",
        "\n",
        "        if args[\"fp16\"]:\n",
        "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, \"dynamic\")\n",
        "\n",
        "        loss_metric = tf.keras.metrics.Mean(name=\"loss\", dtype=tf.float32)\n",
        "        \n",
        "    logging.info(\"***** Running training *****\")\n",
        "    logging.info(\"  Num examples = %d\", num_train_examples)\n",
        "    logging.info(\"  Num Epochs = %d\", args[\"num_train_epochs\"])\n",
        "    logging.info(\"  Instantaneous batch size per device = %d\", args[\"per_device_train_batch_size\"])\n",
        "    logging.info(\n",
        "        \"  Total train batch size (w. parallel, distributed ) = %d\",\n",
        "        train_batch_size,\n",
        "    )\n",
        "\n",
        "    logging.info(\"  Total training steps = %d\", num_train_steps)\n",
        "\n",
        "    model.summary()    \n",
        "\n",
        "    @tf.function\n",
        "    def train_step(train_features, train_labels):\n",
        "        def step_fn(train_features, train_labels):\n",
        "            inputs = {\"attention_mask\": train_features[\"attention_mask\"], \"training\": True}\n",
        "\n",
        "            if args[\"model_type\"] != \"distilbert\":\n",
        "                inputs[\"token_type_ids\"] = (\n",
        "                    train_features[\"token_type_ids\"] if args[\"model_type\"] in [\"bert\", \"xlnet\"] else None\n",
        "                )\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = model(train_features[\"input_ids\"], **inputs)[0]\n",
        "                cross_entropy = loss_fct(train_labels, logits) #per_example_losses\n",
        "                loss = tf.reduce_sum(cross_entropy) * (1.0 / train_batch_size) #per_replica_loss\n",
        "                if args['fp16']:\n",
        "                    scaled_loss = optimizer.get_scaled_loss(loss)\n",
        "\n",
        "            if args['fp16']:\n",
        "              scaled_grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
        "              grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
        "            else:\n",
        "              grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            return cross_entropy\n",
        "\n",
        "        per_example_losses = strategy.experimental_run_v2(step_fn, args=(train_features, train_labels))\n",
        "        sum_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)\n",
        "        \n",
        "        return sum_loss / train_batch_size #loss over replicas\n",
        "\n",
        "    current_time = datetime.datetime.now()\n",
        "\n",
        "    # Train or resume training from chkpt\n",
        "    global_step = 0\n",
        "    epochs_trained = 0\n",
        "    current_step = 0\n",
        "\n",
        "    with strategy.scope():\n",
        "        # Check if continuing training from a checkpoint\n",
        "        checkpoint = tf.train.Checkpoint(step=tf.Variable(0), model=model, optimizer=optimizer)\n",
        "        manager = tf.train.CheckpointManager(checkpoint, f'{args[\"output_dir\"]}/tf_ckpts', max_to_keep=3)\n",
        "        latest_checkpoint_file = manager.latest_checkpoint\n",
        "        if latest_checkpoint_file:\n",
        "          logging.info(\n",
        "              'Checkpoint file %s found and restoring from '\n",
        "              'checkpoint', latest_checkpoint_file)\n",
        "          checkpoint.restore(latest_checkpoint_file)\n",
        "          logging.info('Loading from checkpoint file completed')\n",
        "    if latest_checkpoint_file:\n",
        "        current_step = optimizer.iterations.numpy()\n",
        "        logging.info(\n",
        "              'Resume training from step %s', current_step )\n",
        "    print(\"\\ncurrent step = \", current_step)\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "\n",
        "    epoch_iterator = trange(\n",
        "        epochs_trained, int(args[\"num_train_epochs\"]), desc=\"Epoch\")\n",
        "    global_step = current_step \n",
        "    # global_step without gradient_accumul ==step (useless) \n",
        "    #if multiple epochs global_step may contain steps from multiple epochs, \n",
        "    #step should be step_in_current_epoch, epochs_trained calculated\n",
        "\n",
        "    for epoch in epoch_iterator:\n",
        "        train_iterator = tqdm(train_dataset, total=num_train_steps, desc=\"Iteration\")\n",
        "\n",
        "        with strategy.scope():\n",
        "            for step, (train_features, train_labels) in enumerate(train_iterator):\n",
        "                # Skip past any already trained steps if resuming training\n",
        "                if current_step > 0:\n",
        "                    current_step -= 1\n",
        "                    continue\n",
        "\n",
        "                loss = train_step(train_features, train_labels)\n",
        "                    \n",
        "                loss_metric(loss)\n",
        "\n",
        "                global_step += 1\n",
        "                checkpoint.step.assign_add(1)\n",
        "\n",
        "                if args[\"logging_steps\"] > 0 and global_step % args[\"logging_steps\"] == 0:\n",
        "                        # Log metrics\n",
        "                        lr = optimizer.learning_rate\n",
        "                        learning_rate = lr(step)\n",
        "                        \n",
        "                        logging_loss = loss_metric.result()\n",
        "\n",
        "                        train_iterator.set_postfix(loss=logging_loss.numpy(), step=global_step, lr=learning_rate.numpy())\n",
        "\n",
        "                if args[\"save_steps\"] > 0 and global_step % args[\"save_steps\"] == 0:\n",
        "                        ## TF2 checkpoint for training \n",
        "                        save_path = manager.save()\n",
        "                        logging.info(\"Saved checkpoint for step %s: %s\",global_step,save_path)\n",
        "                        \n",
        "                if args[\"save_steps\"] > 0 and global_step % (args[\"save_steps\"]*10) == 0:\n",
        "                        # Save model checkpoint\n",
        "                        output_dir = os.path.join(args[\"transformer_checkpoints\"], f\"checkpoint-{global_step}\")\n",
        "\n",
        "                        if not os.path.exists(output_dir):\n",
        "                            os.makedirs(output_dir)\n",
        "\n",
        "                        model.save_pretrained(output_dir)\n",
        "                        \n",
        "                        logging.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "                if args['max_steps'] > 0 and global_step > args['max_steps']:\n",
        "                    train_iterator.close()\n",
        "                    break\n",
        "        if args['max_steps'] > 0 and global_step > args['max_steps']:\n",
        "            epoch_iterator.close()\n",
        "            break\n",
        "\n",
        "        epoch_iterator.write(f\"loss epoch {epoch + 1}: {loss_metric.result()}\")\n",
        "\n",
        "        loss_metric.reset_states()\n",
        "\n",
        "    logging.info(\"  Training took time = {}\".format(datetime.datetime.now() - current_time))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TmBXcLBJrgWT",
        "colab": {}
      },
      "source": [
        "def evaluate(args, strategy, eval_dataset, trained_model, num_eval_examples,\n",
        "            eval_batch_size, global_step):\n",
        "\n",
        "    @tf.function\n",
        "    def eval_step(features, labels):\n",
        "      \"\"\"Computes predictions on distributed devices.\"\"\"\n",
        "\n",
        "      def _eval_step_fn(eval_features, labels):\n",
        "        \"\"\"Replicated predictions.\"\"\"\n",
        "        inputs = {\"attention_mask\": eval_features[\"attention_mask\"], \"training\": False}\n",
        "\n",
        "        if args[\"model_type\"] != \"distilbert\":\n",
        "            inputs[\"token_type_ids\"] = (\n",
        "                eval_features[\"token_type_ids\"] if args[\"model_type\"] in [\"bert\", \"xlnet\"] else None\n",
        "            )\n",
        "\n",
        "        logits = trained_model(eval_features[\"input_ids\"], **inputs)[0]\n",
        "\n",
        "        return logits, labels, eval_features['q_id'], eval_features['d_id']\n",
        "\n",
        "      preds, labels, q_id, d_id = strategy.experimental_run_v2(\n",
        "          _eval_step_fn, args=(features, labels))\n",
        "      # outputs: current batch logits as a tuple of shard logits\n",
        "      preds = tf.nest.map_structure(strategy.experimental_local_results,\n",
        "                                      preds)\n",
        "      labels = tf.nest.map_structure(strategy.experimental_local_results, labels)\n",
        "      q_id = tf.nest.map_structure(strategy.experimental_local_results,\n",
        "                                      q_id)\n",
        "      d_id = tf.nest.map_structure(strategy.experimental_local_results,\n",
        "                                      d_id)\n",
        "      return preds, labels, q_id, d_id\n",
        "\n",
        "    \n",
        "    #start eval\n",
        "    num_eval_steps = (\n",
        "            math.ceil(num_eval_examples / eval_batch_size)\n",
        "        )\n",
        "    logging.info(\"***** Running evaluation *****\")\n",
        "    logging.info(\"  Num examples = %d\", num_eval_examples)\n",
        "    logging.info(\"  Instantaneous batch size per device = %d\", \n",
        "                 args[\"per_device_eval_batch_size\"])\n",
        "    logging.info(\n",
        "        \"  Total train batch size (w. parallel, distributed ) = %d\",\n",
        "        eval_batch_size,\n",
        "    )\n",
        "\n",
        "    logging.info(\"  Total evaluation steps = %d\", num_eval_steps)\n",
        "    eval_iterator = tqdm(eval_dataset, total=num_eval_steps, \n",
        "                                   desc=\"Iteration\")\n",
        "\n",
        "    preds = None\n",
        "    golds = None\n",
        "    qids = None\n",
        "    dids= None\n",
        "    msmarco_file = tf.io.gfile.GFile( f\"{args['output_dir']}/predictions_{SET_NAME}_{global_step}.tsv\", 'w')\n",
        "    for step, (eval_features, eval_labels) in enumerate(eval_iterator):\n",
        "        with strategy.scope():\n",
        "            outputs, labels, q_ids, d_ids  = eval_step(eval_features, eval_labels)\n",
        "        \n",
        "        for i in range(args['n_device']):\n",
        "            if preds is None:\n",
        "                preds = outputs[i].numpy()[:,1]\n",
        "                golds = labels[i].numpy()\n",
        "                qids = q_ids[i].numpy()\n",
        "                dids = d_ids[i].numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, outputs[i].numpy()[:,1], axis=0)\n",
        "                golds = np.append(golds, labels[i].numpy(), axis=0)\n",
        "                qids = np.append(qids, q_ids[i].numpy(), axis=0)\n",
        "                dids = np.append(dids, d_ids[i].numpy(), axis=0)\n",
        "                \n",
        "        for qid,did,pred,label in zip(qids,dids,preds,golds):\n",
        "              msmarco_file.write(\"\\t\".join((str(qid), str(did), str(pred), str(label))) + \"\\n\")\n",
        "        preds = None\n",
        "        golds = None\n",
        "        qids = None\n",
        "        dids= None\n",
        "    msmarco_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dgn4zafy13Zr",
        "colab": {}
      },
      "source": [
        "def main(args):\n",
        "    logging.set_verbosity(logging.INFO)\n",
        "\n",
        "    if args[\"fp16\"]:\n",
        "        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
        "\n",
        "    if args[\"tpu\"]:\n",
        "        print(args['tpu'])\n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=args[\"tpu\"])\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)       \n",
        "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "        args[\"n_device\"] = args[\"num_tpu_cores\"]\n",
        "\n",
        "    elif args[\"no_cuda\"]:\n",
        "        args[\"n_device\"] = 1\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "\n",
        "    elif args[\"gpus\"]:\n",
        "        if len(args[\"gpus\"].split(\",\")) > 1:\n",
        "            args[\"n_device\"] = len([f\"/gpu:{gpu}\" for gpu in args[\"gpus\"].split(\",\")])\n",
        "            strategy = tf.distribute.MirroredStrategy(devices=[f\"/gpu:{gpu}\" for gpu in args[\"gpus\"].split(\",\")])\n",
        "        else:\n",
        "            args[\"n_device\"] = len(args[\"gpus\"].split(\",\"))\n",
        "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:\" + args[\"gpus\"].split(\",\")[0])\n",
        "\n",
        "    else:\n",
        "        devices = get_available_gpus()\n",
        "        logging.info(\"\\ndevices= %s \\n\", devices)\n",
        "        args[\"n_device\"] = len(devices)\n",
        "        strategy = tf.distribute.MirroredStrategy(devices=devices)\n",
        "    \n",
        "    logging.warning(\n",
        "        \"n_device: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        args[\"n_device\"],\n",
        "        bool(args[\"n_device\"] > 1),\n",
        "        args[\"fp16\"],\n",
        "    )\n",
        "    \n",
        "    logging.info(\"\\nStrategy = %s\\n\",strategy)\n",
        "\n",
        "    num_labels = 2\n",
        "    tf.random.set_seed(args[\"seed\"])\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args[\"model_type\"]]\n",
        "    config = config_class.from_pretrained(\n",
        "        args[\"config_name\"] if args[\"config_name\"] else args[\"model_name_or_path\"],\n",
        "        num_labels=num_labels,\n",
        "        cache_dir=args[\"cache_dir\"] if args[\"cache_dir\"] else None,\n",
        "    )\n",
        "\n",
        "    logging.info(\"Training/evaluation parameters %s\", args)\n",
        "    \n",
        "    # Training\n",
        "    if args[\"do_train\"]:\n",
        "        # tokenizer = tokenizer_class.from_pretrained(\n",
        "        #     args[\"tokenizer_name\"] if args[\"tokenizer_name\"] else args[\"model_name_or_path\"],\n",
        "        #     do_lower_case=args[\"do_lower_case\"],\n",
        "        #     cache_dir=args[\"cache_dir\"] if args[\"cache_dir\"] else None,\n",
        "        # )\n",
        "\n",
        "        with strategy.scope():\n",
        "            model = model_class.from_pretrained(\n",
        "                args[\"model_name_or_path\"],\n",
        "                from_pt=bool(\".bin\" in args[\"model_name_or_path\"]),\n",
        "                config=config,\n",
        "                cache_dir=args[\"cache_dir\"] if args[\"cache_dir\"] else None,\n",
        "            )\n",
        "            model.layers[-1].activation = tf.keras.activations.softmax\n",
        "\n",
        "        train_batch_size = args[\"per_device_train_batch_size\"] * max(1,args[\"n_device\"])\n",
        "\n",
        "        filename = tf.io.gfile.glob(f\"{args['data_dir']}/{FILE_NAME}\")\n",
        "        train_dataset, num_train_examples = get_dataset( filename,\n",
        "                train_batch_size, args[\"max_seq_length\"], is_training_set=True\n",
        "        )\n",
        "        train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "        train(\n",
        "            args,\n",
        "            strategy,\n",
        "            train_dataset,\n",
        "            model,\n",
        "            num_train_examples,\n",
        "            train_batch_size,\n",
        "        )\n",
        "\n",
        "        if not os.path.exists(args[\"transformer_checkpoints\"]):\n",
        "            os.makedirs(args[\"transformer_checkpoints\"])\n",
        "\n",
        "        logging.info(\"Saving model to %s\", args[\"transformer_checkpoints\"])\n",
        "\n",
        "        model.save_pretrained(args[\"transformer_checkpoints\"])\n",
        "        #tokenizer.save_pretrained(args[\"transformer_checkpoints\"])\n",
        "    # Evaluating\n",
        "    if args[\"do_eval\"]:\n",
        "        checkpoints = []\n",
        "\n",
        "        if args[\"eval_all_checkpoints\"]:\n",
        "            checkpoints = list(\n",
        "                os.path.dirname(c)\n",
        "                for c in sorted(\n",
        "                    glob.glob(args[\"transformer_checkpoints\"] + \"/**/\" + TF2_WEIGHTS_NAME, recursive=True),\n",
        "                    key=lambda f: int(\"\".join(filter(str.isdigit, f)) or -1),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if len(checkpoints) == 0:\n",
        "            ##last checkpoint\n",
        "            checkpoints = list(\n",
        "                  os.path.dirname(c)\n",
        "                  for c in sorted(\n",
        "                      glob.glob(args[\"transformer_checkpoints\"] + \"/**/\" + TF2_WEIGHTS_NAME, recursive=True),\n",
        "                      key=lambda f: int(\"\".join(filter(str.isdigit, f)) or -1),\n",
        "                  )\n",
        "              )\n",
        "            checkpoints = [checkpoints[-1]]\n",
        "            \n",
        "            # checkpoints.append(args[\"transformer_checkpoints\"])\n",
        "        \n",
        "        logging.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        \n",
        "\n",
        "        # modified section\n",
        "        tokenizer = tokenizer_class.from_pretrained(\n",
        "            args[\"tokenizer_name\"] if args[\"tokenizer_name\"] else args[\"model_name_or_path\"],\n",
        "            do_lower_case=args[\"do_lower_case\"],\n",
        "            cache_dir=args[\"cache_dir\"] if args[\"cache_dir\"] else None,\n",
        "        )\n",
        "\n",
        "        marker = get_marker(STRATEGY.lower())\n",
        "\n",
        "        handle = DocumentSplitterHandle(tokenizer, args['max_seq_length'])\n",
        "        doc_processor = Robust04Processor(handle,marker)\n",
        "\n",
        "        eval_batch_size = args[\"per_device_eval_batch_size\"] * max(1,args[\"n_device\"])\n",
        "\n",
        "        filename = tf.io.gfile.glob(f\"{args['data_dir']}/{FILE_NAME}\")\n",
        "        eval_dataset, num_eval_examples = doc_processor.get_eval_dataset(filename, eval_batch_size)\n",
        "        print('num_eval_examples= ', num_eval_examples)\n",
        "        eval_dataset = strategy.experimental_distribute_dataset(eval_dataset)\n",
        "\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1] if re.match(\".*checkpoint-[0-9]\", checkpoint) else \"final\"\n",
        "            print(\"global step= \", global_step)\n",
        "\n",
        "            with strategy.scope():\n",
        "                trained_model = model_class.from_pretrained(checkpoint)\n",
        "                trained_model.summary()\n",
        "        \n",
        "            evaluate(\n",
        "                args,\n",
        "                strategy,\n",
        "                eval_dataset,\n",
        "                trained_model,\n",
        "                num_eval_examples,\n",
        "                eval_batch_size,\n",
        "                global_step,\n",
        "            )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvAZ2AHC1KQi",
        "colab_type": "code",
        "outputId": "5fd35f9d-bd52-4ad0-af33-0a04c93f233b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a01d7d4297964b4d9e3cc364e7e7c51a",
            "caa814dca3604a3297ef210de11c61c0",
            "1c8fc63ced814ba5ad1226d2f2bae45e",
            "c3164d6976034372a7a1f4f32e78e891",
            "d07c37e86ba74be3b9be5f63cdbfb3be",
            "e7e96b7243b647318c1fb7f2e0bc83e3",
            "73082573ee12488e90947feba7ef694f",
            "e3be2f77456b43f183f8535f9fb2ec09"
          ]
        }
      },
      "source": [
        "main(args)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "grpc://10.6.75.98:8470\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.6.75.98:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.6.75.98:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "WARNING:absl:n_device: 8, distributed training: True, 16-bits training: False\n",
            "INFO:absl:\n",
            "Strategy = <tensorflow.python.distribute.tpu_strategy.TPUStrategy object at 0x7f6ae46b2b00>\n",
            "\n",
            "INFO:absl:Training/evaluation parameters {'data_dir': 'gs://mbougha2/trec_test_set', 'model_type': 'bert', 'model_name_or_path': 'bert-base-uncased', 'output_dir': 'gs://mbougha2/output_dir/', 'transformer_checkpoints': '/content/drive/My Drive/MyColabNotebooks/MarkedBERT/checkpoint', 'max_seq_length': 512, 'tpu': 'grpc://10.6.75.98:8470', 'do_train': False, 'do_eval': True, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 4, 'max_steps': 100000, 'warmup_steps': 10000, 'learning_rate': 3e-06, 'adam_epsilon': 1e-08, 'logging_steps': 100, 'seed': 42, 'max_grad_norm': 1.0, 'save_steps': 5000, 'overwrite_output_dir': False, 'fp16': False, 'no_cuda': False, 'gpus': None, 'config_name': None, 'cache_dir': None, 'tokenizer_name': None, 'num_tpu_cores': 8, 'do_predict': False, 'evaluate_during_training': False, 'do_lower_case': False, 'num_train_epochs': 1, 'overwrite_cache': False, 'eval_all_checkpoints': False, 'msmarco_output': True, 'num_eval_docs': 1000, 'n_device': 8}\n",
            "INFO:absl:Evaluate the following checkpoints: ['/content/drive/My Drive/MyColabNotebooks/MarkedBERT/checkpoint']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_eval_examples=  41656\n",
            "global step=  final\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:***** Running evaluation *****\n",
            "INFO:absl:  Num examples = 41656\n",
            "INFO:absl:  Instantaneous batch size per device = 4\n",
            "INFO:absl:  Total train batch size (w. parallel, distributed ) = 32\n",
            "INFO:absl:  Total evaluation steps = 1302\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  109482240 \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "=================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a01d7d4297964b4d9e3cc364e7e7c51a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Iteration', max=1302, style=ProgressStyle(description_width='â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b3f47c9a9b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-abd1fe079d26>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnum_eval_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0meval_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             )\n",
            "\u001b[0;32m<ipython-input-14-97550fb8a907>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(args, strategy, eval_dataset, trained_model, num_eval_examples, eval_batch_size, global_step)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meval_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ids\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: in user code:\n\n    <ipython-input-14-97550fb8a907>:19 _eval_step_fn  *\n        return logits, labels, eval_features['q_id'], eval_features['d_id']\n\n    KeyError: 'q_id'\n"
          ]
        }
      ]
    }
  ]
}